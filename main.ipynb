{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94fd7617-172f-4297-8694-381c53666049",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import gc\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from functools import partial\n",
    "from scipy.special import softmax\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import LlamaTokenizer, AutoTokenizer, AutoModelForCausalLM\n",
    "import pickle\n",
    "\n",
    "import logging\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2616a98f-eeb8-45eb-94ea-d8a6a4ef9ce9",
   "metadata": {},
   "source": [
    "# Huggingface Login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95cddd1b-ddb4-479c-b5b9-3e6c0883bf3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paste your Hugging Face token:  ········\n",
      "✅ Logged in to Hugging Face Hub.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Logged in to Hugging Face Hub.\n"
     ]
    }
   ],
   "source": [
    "from getpass import getpass\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Paste your token when prompted (e.g., starts with \"hf_...\")\n",
    "token = getpass(\"Paste your Hugging Face token: \")\n",
    "login(token=token, add_to_git_credential=True)  # stores token for future use\n",
    "\n",
    "print(\"✅ Logged in to Hugging Face Hub.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3da124-ec09-436d-b520-e2ce4514f265",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b6b1c34-ca74-4d04-b2ce-124e5312bb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "LLAMA_CONFIG = {\n",
    "    # Dataset configuration\n",
    "    'dataset_path': 'data/SciQ/test.csv',  # Change this to your dataset path\n",
    "    'dataset_name': 'SciQ',  # For tracking purposes\n",
    "    \n",
    "    # Model configuration\n",
    "    'model_name': 'daryl149/llama-2-7b-chat-hf',\n",
    "    'torch_dtype': torch.bfloat16,\n",
    "    'device': 'cuda',\n",
    "    \n",
    "    # Evaluation configuration\n",
    "    'get_sample': False,  # Set to True to test on single sample\n",
    "    'sample_index': 0,    # Which sample to test if get_sample is True\n",
    "    'batch_size': 4,      # For STL approach\n",
    "    \n",
    "    # Prompt templates\n",
    "    'baseline_prompt': \"\"\"<s> [INST] Your task is to analyze the question and answer options A, B, C or D below.\n",
    "{query}\n",
    "[/INST] Answer:\"\"\",\n",
    "    \n",
    "    'stl_prompt': \"\"\"<s> [INST] Your task is to analyze the question and answer below. If the answer is correct, respond yes, if it is not correct respond no.\n",
    "{question} [/INST]\"\"\",\n",
    "    \n",
    "    'cot_prompt': \"\"\"<s> [INST] Your task is to analyze the question and answer options A, B, C or D below. Let's think step by step and provide your reasoning, then give your final answer in the format \"Answer: [A/B/C/D]\".\n",
    "\n",
    "{query}\n",
    "[/INST] Let me think step by step:\n",
    "\n",
    "\"\"\",\n",
    "    \n",
    "    # Generation parameters\n",
    "    'max_new_tokens_baseline': 1,\n",
    "    'max_new_tokens_stl': 1,\n",
    "    'max_new_tokens_cot': 2000,\n",
    "    \n",
    "    # Output configuration\n",
    "    'save_results': True,\n",
    "    'results_dir': 'results',\n",
    "}\n",
    "\n",
    "\n",
    "MISTRAL_CONFIG = {\n",
    "    # Dataset configuration\n",
    "    'dataset_path': 'data/SciQ/test.csv',\n",
    "    'dataset_name': 'SciQ',\n",
    "    \n",
    "    # Model configuration\n",
    "    'model_name': 'mistralai/Mistral-7B-Instruct-v0.1',\n",
    "    'torch_dtype': torch.bfloat16,\n",
    "    'device': 'cuda',\n",
    "    \n",
    "    # Evaluation configuration\n",
    "    'get_sample': False,\n",
    "    'sample_index': 0,\n",
    "    'batch_size': 4,\n",
    "    \n",
    "    # Prompt templates for Mistral\n",
    "    'baseline_prompt': \"\"\"<s>[INST] Your task is to analyze the question and answer options A, B, C or D below.\n",
    "{query} [/INST]Answer:\"\"\",\n",
    "    \n",
    "    'stl_prompt': \"\"\"<s>[INST] Your task is to analyze the question and answer below. If the answer is correct, respond yes, if it is not correct respond no.\n",
    "{question} [/INST]\"\"\",\n",
    "    \n",
    "    'cot_prompt': \"\"\"<s>[INST] Your task is to analyze the question and answer options A, B, C or D below. Let's think step by step and provide your reasoning, then give your final answer in the format \"Answer: [A/B/C/D]\".\n",
    "\n",
    "{query} [/INST]Let me think step by step:\n",
    "\n",
    "\"\"\",\n",
    "    \n",
    "    # Generation parameters\n",
    "    'max_new_tokens_baseline': 1,\n",
    "    'max_new_tokens_stl': 1,\n",
    "    'max_new_tokens_cot': 2000,\n",
    "    \n",
    "    # Output configuration\n",
    "    'save_results': True,\n",
    "    'results_dir': 'results',\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd7256c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  dataset_path: data/SciQ/test.csv\n",
      "  dataset_name: SciQ\n",
      "  model_name: daryl149/llama-2-7b-chat-hf\n",
      "  torch_dtype: torch.bfloat16\n",
      "  device: cuda\n",
      "  get_sample: False\n",
      "  sample_index: 0\n",
      "  batch_size: 4\n",
      "  baseline_prompt: [Template defined]\n",
      "  stl_prompt: [Template defined]\n",
      "  cot_prompt: [Template defined]\n",
      "  max_new_tokens_baseline: 1\n",
      "  max_new_tokens_stl: 1\n",
      "  max_new_tokens_cot: 2000\n",
      "  save_results: True\n",
      "  results_dir: results\n"
     ]
    }
   ],
   "source": [
    "CONFIG = LLAMA_CONFIG\n",
    "\n",
    "# Create results directory\n",
    "os.makedirs(CONFIG['results_dir'], exist_ok=True)\n",
    "\n",
    "print(\"Configuration:\")\n",
    "for key, value in CONFIG.items():\n",
    "    if 'prompt' in key:\n",
    "        print(f\"  {key}: [Template defined]\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188de07c-d446-495c-8824-af88518e4f1d",
   "metadata": {},
   "source": [
    "# Load model & dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef6c8ab-b42b-492f-983b-d725e284d2f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer: daryl149/llama-2-7b-chat-hf\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d88e9f6b7e1d4b40b6cfce59b743e992",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/727 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\AppData\\Roaming\\Python\\Python312\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\PC\\.cache\\huggingface\\hub\\models--daryl149--llama-2-7b-chat-hf. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "880769199cab40bda907cccea146eb5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a260722689f5460b9b9cce1f6664110c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79dae97ae3b24c47811e0149ca967888",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/411 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded successfully\n"
     ]
    }
   ],
   "source": [
    "def load_tokenizer(model_name):\n",
    "    \"\"\"Load tokenizer only\"\"\"\n",
    "    print(f\"Loading tokenizer: {model_name}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    # Setup tokenizer for batch processing\n",
    "    tokenizer.padding_side = \"left\"\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    print(\"Tokenizer loaded successfully\")\n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "def load_model(model_name, torch_dtype, device='cuda'):\n",
    "    \"\"\"Load model only\"\"\"\n",
    "    print(f\"Loading model: {model_name}\")\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch_dtype,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Ensure pad_token is set\n",
    "    if model.config.pad_token_id is None:\n",
    "        model.config.pad_token_id = model.config.eos_token_id\n",
    "    \n",
    "    print(f\"Model loaded successfully on {device}\")\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "tokenizer = load_tokenizer(CONFIG['model_name'])\n",
    "\n",
    "model = load_model(CONFIG['model_name'], CONFIG['torch_dtype'], CONFIG['device'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d2e8123-01c4-4882-84d9-848f55563b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels token IDs: [319, 350, 315, 360]\n",
      "Yes token IDs: [4874]\n",
      "No token IDs: [694]\n"
     ]
    }
   ],
   "source": [
    "def get_token_ids(tokenizer):\n",
    "    \"\"\"Get token IDs for labels and yes/no tokens\"\"\"\n",
    "    labels = [\"A\", \"B\", \"C\", \"D\"]\n",
    "    labels_token_id = tokenizer.encode(\"A B C D\")[1:]  # Remove BOS token\n",
    "    yes_token_id = tokenizer.encode(\"yes\")[1:]\n",
    "    no_token_id = tokenizer.encode(\"no\")[1:]\n",
    "    \n",
    "    print(f\"Labels token IDs: {labels_token_id}\")\n",
    "    print(f\"Yes token IDs: {yes_token_id}\")\n",
    "    print(f\"No token IDs: {no_token_id}\")\n",
    "    \n",
    "    return labels, labels_token_id, yes_token_id, no_token_id\n",
    "\n",
    "# Get token IDs\n",
    "labels, labels_token_id, yes_token_id, no_token_id = get_token_ids(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabdd542-7851-4bd8-865b-982efc6e7f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(dataset_path, get_sample=False, sample_index=0):\n",
    "    \"\"\"Load and prepare dataset\"\"\"\n",
    "    print(f\"Loading dataset from: {dataset_path}\")\n",
    "    \n",
    "    if not os.path.exists(dataset_path):\n",
    "        raise FileNotFoundError(f\"Dataset not found at {dataset_path}\")\n",
    "    \n",
    "    df = pd.read_csv(dataset_path)\n",
    "    \n",
    "    # Handle different dataset formats\n",
    "    if 'id' in df.columns:\n",
    "        df = df.drop(\"id\", axis=1)\n",
    "    \n",
    "    df.fillna(' ', inplace=True)\n",
    "    df = df.astype(str)\n",
    "    \n",
    "    if get_sample:\n",
    "        df = df.iloc[[sample_index]]\n",
    "        print(f\"Using sample at index {sample_index}\")\n",
    "    \n",
    "    # Create instruction column for baseline and CoT approaches\n",
    "    df['instruction'] = 'Question: ' + df['question'] + '\\n\\nA. ' + df['A'] + '\\n\\nB. ' + df['B'] + '\\n\\nC. ' + df['C'] + ' \\n\\nD. ' + df['D']\n",
    "    \n",
    "    print(f\"Dataset loaded: {len(df)} samples\")\n",
    "    print(\"\\nSample question:\")\n",
    "    print(f\"Question: {df['question'].iloc[0]}\")\n",
    "    print(f\"A. {df['A'].iloc[0]}\")\n",
    "    print(f\"B. {df['B'].iloc[0]}\")\n",
    "    print(f\"C. {df['C'].iloc[0]}\")\n",
    "    print(f\"D. {df['D'].iloc[0]}\")\n",
    "    print(f\"Answer: {df['answer'].iloc[0]}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load dataset\n",
    "df = load_dataset(CONFIG['dataset_path'], CONFIG['get_sample'], CONFIG['sample_index'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a41a789-78f3-4701-b9f3-60a6cf1dc671",
   "metadata": {},
   "source": [
    "# Baseline Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107f985d-500a-4c7e-8ae8-bbba968ddaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens(tokenizer, text):\n",
    "    \"\"\"Count tokens in text\"\"\"\n",
    "    return len(tokenizer.encode(text))\n",
    "\n",
    "def baseline_approach(model, tokenizer, df, labels_token_id, config):\n",
    "    \"\"\"Baseline approach: Direct answer prediction\"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"BASELINE APPROACH\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    results = []\n",
    "    preds = []\n",
    "    logits_list = []\n",
    "    \n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing baseline\"):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Prepare input using configurable prompt\n",
    "        full_prompt = config['baseline_prompt'].format(query=row['instruction'])\n",
    "        input_tokens = count_tokens(tokenizer, full_prompt)\n",
    "        \n",
    "        inputs = tokenizer(full_prompt, return_tensors=\"pt\").to(f\"cuda:{model.device.index}\")\n",
    "        \n",
    "        # Generate prediction\n",
    "        with torch.no_grad():\n",
    "            output = model.generate(\n",
    "                input_ids=inputs[\"input_ids\"], \n",
    "                attention_mask=inputs[\"attention_mask\"],\n",
    "                max_new_tokens=config['max_new_tokens_baseline'],\n",
    "                return_dict_in_generate=True, \n",
    "                output_scores=True\n",
    "            )\n",
    "        \n",
    "        # Process logits\n",
    "        first_token_logits = output.scores[0][0]\n",
    "        option_logits = first_token_logits[labels_token_id].float().cpu().numpy()\n",
    "        logits_list.append(option_logits)\n",
    "        \n",
    "        # Get prediction\n",
    "        pred = np.array([\"A\", \"B\", \"C\", \"D\"])[np.argsort(option_logits)[::-1][:4]]\n",
    "        pred = ' '.join(pred)\n",
    "        preds.append(pred)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        end_time = time.time()\n",
    "        latency = end_time - start_time\n",
    "        output_tokens = config['max_new_tokens_baseline']\n",
    "        \n",
    "        # Record results\n",
    "        results.append({\n",
    "            'question_id': idx,\n",
    "            'approach': 'baseline',\n",
    "            'question': row['question'][:100] + '...' if len(row['question']) > 100 else row['question'],\n",
    "            'true_answer': row['answer'],\n",
    "            'predicted_answer': pred.split()[0],\n",
    "            'full_prediction': pred,\n",
    "            'input_tokens': input_tokens,\n",
    "            'output_tokens': output_tokens,\n",
    "            'latency_seconds': latency,\n",
    "            'logits': option_logits.tolist(),\n",
    "            'probabilities': softmax(option_logits).tolist()\n",
    "        })\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    first_preds = [pred.split()[0] for pred in preds]\n",
    "    accuracy = sum(1 for i, pred in enumerate(first_preds) if pred == df.iloc[i]['answer']) / len(df)\n",
    "    \n",
    "    print(f\"\\nBaseline Results:\")\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"Average latency: {np.mean([r['latency_seconds'] for r in results]):.3f}s\")\n",
    "    print(f\"Average input tokens: {np.mean([r['input_tokens'] for r in results]):.1f}\")\n",
    "    \n",
    "    return results, accuracy\n",
    "\n",
    "# Run baseline approach\n",
    "baseline_results, baseline_accuracy = baseline_approach(model, tokenizer, df, labels_token_id, CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758005e6-2f69-4ef7-a294-85731111c298",
   "metadata": {},
   "source": [
    "# STL Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860baf4f-57b0-49ba-89dc-0ca3dca04603",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stl_prompts(row, config):\n",
    "    \"\"\"Generate prompts for Single Token Logits approach\"\"\"\n",
    "    base_prompt = config['stl_prompt']\n",
    "    question = f\"\\nQuestion: {row['question']}\\nProposed answer: \"\n",
    "    \n",
    "    prompts = []\n",
    "    for letter in \"ABCD\":\n",
    "        prompt_suffix = f\"{row[letter]}\\n\\n### Response:\\n\"\n",
    "        full_prompt = base_prompt.format(question=question) + prompt_suffix\n",
    "        prompts.append(full_prompt)\n",
    "    \n",
    "    return prompts\n",
    "\n",
    "def single_token_logits_approach(model, tokenizer, df, yes_token_id, no_token_id, config):\n",
    "    \"\"\"Single Token Logits approach\"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"SINGLE TOKEN LOGITS APPROACH\") \n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Prepare all prompts\n",
    "    f = partial(get_stl_prompts, config=config)\n",
    "    all_prompts = df.apply(f, axis=1).values\n",
    "    all_prompts = [item for sublist in all_prompts for item in sublist]\n",
    "    \n",
    "    results = []\n",
    "    yes_logits_all = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(all_prompts), config['batch_size']), desc=\"Processing STL batches\"):\n",
    "            batch_start_time = time.time()\n",
    "            \n",
    "            batch = all_prompts[i:i+config['batch_size']]\n",
    "            \n",
    "            # Count input tokens for each prompt in batch\n",
    "            input_tokens_batch = [count_tokens(tokenizer, prompt) for prompt in batch]\n",
    "            \n",
    "            batch_tokens = tokenizer(batch, return_tensors=\"pt\", return_attention_mask=True, padding=True).to(f\"cuda:{model.device.index}\")\n",
    "            \n",
    "            batch_outputs = model.generate(\n",
    "                **batch_tokens,\n",
    "                max_new_tokens=config['max_new_tokens_stl'],\n",
    "                return_dict_in_generate=True,\n",
    "                output_scores=True,\n",
    "            )\n",
    "            \n",
    "            first_token_logits = batch_outputs.scores[0]\n",
    "            batch_end_time = time.time()\n",
    "            batch_latency = batch_end_time - batch_start_time\n",
    "            \n",
    "            # Process each item in batch\n",
    "            for j, scores in enumerate(first_token_logits):\n",
    "                yes_logit = float(scores[yes_token_id].float().cpu().numpy())\n",
    "                no_logit = float(scores[no_token_id].float().cpu().numpy())\n",
    "                yes_logits_all.append(yes_logit)\n",
    "                \n",
    "                # Calculate which question this belongs to\n",
    "                question_idx = (i + j) // 4\n",
    "                option_idx = (i + j) % 4\n",
    "                \n",
    "                if question_idx < len(df):  # Safety check\n",
    "                    results.append({\n",
    "                        'question_id': question_idx,\n",
    "                        'option': ['A', 'B', 'C', 'D'][option_idx],\n",
    "                        'yes_logit': yes_logit,\n",
    "                        'no_logit': no_logit,\n",
    "                        'input_tokens': input_tokens_batch[j],\n",
    "                        'output_tokens': config['max_new_tokens_stl'],\n",
    "                        'latency_seconds': batch_latency / len(batch)  # Distribute batch latency\n",
    "                    })\n",
    "            \n",
    "            # Cleanup\n",
    "            del batch_tokens\n",
    "            del batch_outputs\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    # Reshape results by question\n",
    "    yes_logits_array = np.array(yes_logits_all)\n",
    "    reshaped_logits = np.reshape(yes_logits_array, (-1, 4))\n",
    "    probs_output = np.apply_along_axis(softmax, 1, reshaped_logits)\n",
    "    \n",
    "    # Generate predictions\n",
    "    labels = np.array([\"A\", \"B\", \"C\", \"D\"])\n",
    "    preds = []\n",
    "    question_results = []\n",
    "    \n",
    "    for q_idx, (option_logits, option_probs) in enumerate(zip(reshaped_logits, probs_output)):\n",
    "        pred = labels[np.argsort(option_logits)[::-1][:4]]\n",
    "        pred = ' '.join(pred)\n",
    "        preds.append(pred)\n",
    "        \n",
    "        # Get question-level metrics\n",
    "        question_results.append({\n",
    "            'question_id': q_idx,\n",
    "            'approach': 'stl',\n",
    "            'question': df.iloc[q_idx]['question'][:100] + '...' if len(df.iloc[q_idx]['question']) > 100 else df.iloc[q_idx]['question'],\n",
    "            'true_answer': df.iloc[q_idx]['answer'],\n",
    "            'predicted_answer': pred.split()[0],\n",
    "            'full_prediction': pred,\n",
    "            'input_tokens': np.mean([r['input_tokens'] for r in results if r['question_id'] == q_idx]),\n",
    "            'output_tokens': 4,  # 4 options, 1 token each\n",
    "            'latency_seconds': sum([r['latency_seconds'] for r in results if r['question_id'] == q_idx]),\n",
    "            'yes_logits': option_logits.tolist(),\n",
    "            'probabilities': option_probs.tolist()\n",
    "        })\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    first_preds = [pred.split()[0] for pred in preds]\n",
    "    accuracy = sum(1 for i, pred in enumerate(first_preds) if pred == df.iloc[i]['answer']) / len(df)\n",
    "    \n",
    "    print(f\"\\nSTL Results:\")\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"Average latency: {np.mean([r['latency_seconds'] for r in question_results]):.3f}s\")\n",
    "    print(f\"Average input tokens: {np.mean([r['input_tokens'] for r in question_results]):.1f}\")\n",
    "    \n",
    "    return question_results, accuracy\n",
    "\n",
    "# Run STL approach\n",
    "stl_results, stl_accuracy = single_token_logits_approach(model, tokenizer, df, yes_token_id, no_token_id, CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e859aab-d2ba-42cc-9159-5f91d108e1f3",
   "metadata": {},
   "source": [
    "# CoT Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5db170-e15e-45d3-9920-7b3e6fb7491d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_answer_from_cot(text):\n",
    "    \"\"\"Extract answer from Chain-of-Thought response\"\"\"\n",
    "    # Look for patterns like \"Answer: A\", \"Answer:A\", \"answer: B\", etc.\n",
    "    patterns = [\n",
    "        r\"Answer:\\s*([ABCD])\",\n",
    "        r\"answer:\\s*([ABCD])\",\n",
    "        r\"Answer\\s*:\\s*([ABCD])\",\n",
    "        r\"answer\\s*:\\s*([ABCD])\",\n",
    "        r\"The answer is\\s*([ABCD])\",\n",
    "        r\"the answer is\\s*([ABCD])\",\n",
    "        r\"Therefore,?\\s*([ABCD])\",\n",
    "        r\"therefore,?\\s*([ABCD])\",\n",
    "        r\"\\b([ABCD])\\s*is\\s*correct\",\n",
    "        r\"correct answer is\\s*([ABCD])\",\n",
    "        r\"([ABCD])\\s*$\"  # Single letter at end\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, text, re.IGNORECASE)\n",
    "        if match:\n",
    "            return match.group(1).upper()\n",
    "    \n",
    "    # If no pattern found, look for the most frequent A, B, C, D in the text\n",
    "    letters = re.findall(r'\\b([ABCD])\\b', text)\n",
    "    if letters:\n",
    "        from collections import Counter\n",
    "        most_common = Counter(letters).most_common(1)[0][0]\n",
    "        return most_common\n",
    "    \n",
    "    # Default fallback\n",
    "    return \"A\"\n",
    "\n",
    "def chain_of_thought_approach(model, tokenizer, df, config):\n",
    "    \"\"\"Chain-of-Thought approach: Generate reasoning then answer\"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"CHAIN-OF-THOUGHT APPROACH\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    results = []\n",
    "    preds = []\n",
    "    \n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing CoT\"):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Prepare input using configurable prompt\n",
    "        full_prompt = config['cot_prompt'].format(query=row['instruction'])\n",
    "        input_tokens = count_tokens(tokenizer, full_prompt)\n",
    "        \n",
    "        inputs = tokenizer(full_prompt, return_tensors=\"pt\").to(f\"cuda:{model.device.index}\")\n",
    "        \n",
    "        # Generate prediction with reasoning\n",
    "        with torch.no_grad():\n",
    "            output = model.generate(\n",
    "                input_ids=inputs[\"input_ids\"], \n",
    "                attention_mask=inputs[\"attention_mask\"],\n",
    "                max_new_tokens=config['max_new_tokens_cot'],\n",
    "                do_sample=True,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        # Decode the generated text\n",
    "        generated_ids = output[0][len(inputs[\"input_ids\"][0]):]\n",
    "        generated_text = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "        \n",
    "        # Extract answer from the generated text\n",
    "        predicted_answer = extract_answer_from_cot(generated_text)\n",
    "        preds.append(predicted_answer)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        end_time = time.time()\n",
    "        latency = end_time - start_time\n",
    "        output_tokens = len(generated_ids)\n",
    "        \n",
    "        # Record results\n",
    "        results.append({\n",
    "            'question_id': idx,\n",
    "            'approach': 'cot',\n",
    "            'question': row['question'][:100] + '...' if len(row['question']) > 100 else row['question'],\n",
    "            'true_answer': row['answer'],\n",
    "            'predicted_answer': predicted_answer,\n",
    "            'full_prediction': generated_text[:500] + '...' if len(generated_text) > 500 else generated_text,\n",
    "            'reasoning': generated_text,\n",
    "            'input_tokens': input_tokens,\n",
    "            'output_tokens': output_tokens,\n",
    "            'latency_seconds': latency,\n",
    "            'logits': [],  # Not applicable for CoT\n",
    "            'probabilities': []  # Not applicable for CoT\n",
    "        })\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = sum(1 for i, pred in enumerate(preds) if pred == df.iloc[i]['answer']) / len(df)\n",
    "    \n",
    "    print(f\"\\nCoT Results:\")\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"Average latency: {np.mean([r['latency_seconds'] for r in results]):.3f}s\")\n",
    "    print(f\"Average input tokens: {np.mean([r['input_tokens'] for r in results]):.1f}\")\n",
    "    print(f\"Average output tokens: {np.mean([r['output_tokens'] for r in results]):.1f}\")\n",
    "    \n",
    "    return results, accuracy\n",
    "\n",
    "# Run CoT approach\n",
    "cot_results, cot_accuracy = chain_of_thought_approach(model, tokenizer, df, CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb08149c-502b-4304-b31f-033fe49b0817",
   "metadata": {},
   "source": [
    "# Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bea85cc-e26b-4b3b-95ac-c0c7100b8d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(baseline_results, stl_results, cot_results, config):\n",
    "    \"\"\"Save results to files\"\"\"\n",
    "    if not config['save_results']:\n",
    "        return\n",
    "    \n",
    "    # Combine all results\n",
    "    all_results = baseline_results + stl_results + cot_results\n",
    "    results_df = pd.DataFrame(all_results)\n",
    "    \n",
    "    # Create filename with model and dataset info\n",
    "    model_name = config['model_name'].replace('/', '_')\n",
    "    dataset_name = config['dataset_name']\n",
    "    timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    filename = f\"{config['results_dir']}/results_{model_name}_{dataset_name}_{timestamp}.csv\"\n",
    "    results_df.to_csv(filename, index=False)\n",
    "    \n",
    "    print(f\"\\nResults saved to: {filename}\")\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\nSummary:\")\n",
    "    print(f\"Dataset: {dataset_name}\")\n",
    "    print(f\"Model: {config['model_name']}\")\n",
    "    print(f\"Total questions: {len(baseline_results)}\")\n",
    "    print(f\"Baseline accuracy: {baseline_accuracy * 100:.2f}%\")\n",
    "    print(f\"STL accuracy: {stl_accuracy * 100:.2f}%\")\n",
    "    print(f\"CoT accuracy: {cot_accuracy * 100:.2f}%\")\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "# Save results\n",
    "results_df = save_results(baseline_results, stl_results, cot_results, CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eca860a-1639-4a35-a8da-5ea561c640de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample results\n",
    "if len(baseline_results) > 0:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"SAMPLE RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    sample_idx = 0\n",
    "    baseline_sample = baseline_results[sample_idx]\n",
    "    stl_sample = stl_results[sample_idx]\n",
    "    cot_sample = cot_results[sample_idx]\n",
    "    \n",
    "    print(f\"\\nQuestion: {baseline_sample['question']}\")\n",
    "    print(f\"True Answer: {baseline_sample['true_answer']}\")\n",
    "    \n",
    "    print(f\"\\nBaseline:\")\n",
    "    print(f\"  Prediction: {baseline_sample['predicted_answer']}\")\n",
    "    print(f\"  Latency: {baseline_sample['latency_seconds']:.3f}s\")\n",
    "    print(f\"  Input tokens: {baseline_sample['input_tokens']}\")\n",
    "    print(f\"  Probabilities: {[f'{p:.3f}' for p in baseline_sample['probabilities']]}\")\n",
    "    \n",
    "    print(f\"\\nSTL:\")\n",
    "    print(f\"  Prediction: {stl_sample['predicted_answer']}\")\n",
    "    print(f\"  Latency: {stl_sample['latency_seconds']:.3f}s\")\n",
    "    print(f\"  Input tokens: {stl_sample['input_tokens']:.1f}\")\n",
    "    print(f\"  Probabilities: {[f'{p:.3f}' for p in stl_sample['probabilities']]}\")\n",
    "    \n",
    "    print(f\"\\nCoT:\")\n",
    "    print(f\"  Prediction: {cot_sample['predicted_answer']}\")\n",
    "    print(f\"  Latency: {cot_sample['latency_seconds']:.3f}s\")\n",
    "    print(f\"  Input/Output tokens: {cot_sample['input_tokens']}/{cot_sample['output_tokens']}\")\n",
    "    print(f\"  Reasoning: {cot_sample['reasoning'][:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c58478-491b-42f2-8e28-8d1b04dbee15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance comparison\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PERFORMANCE COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "approaches = ['Baseline', 'STL', 'CoT']\n",
    "accuracies = [baseline_accuracy * 100, stl_accuracy * 100, cot_accuracy * 100]\n",
    "avg_latencies = [\n",
    "    np.mean([r['latency_seconds'] for r in baseline_results]),\n",
    "    np.mean([r['latency_seconds'] for r in stl_results]),\n",
    "    np.mean([r['latency_seconds'] for r in cot_results])\n",
    "]\n",
    "avg_input_tokens = [\n",
    "    np.mean([r['input_tokens'] for r in baseline_results]),\n",
    "    np.mean([r['input_tokens'] for r in stl_results]),\n",
    "    np.mean([r['input_tokens'] for r in cot_results])\n",
    "]\n",
    "avg_output_tokens = [\n",
    "    np.mean([r['output_tokens'] for r in baseline_results]),\n",
    "    np.mean([r['output_tokens'] for r in stl_results]),\n",
    "    np.mean([r['output_tokens'] for r in cot_results])\n",
    "]\n",
    "\n",
    "print(f\"{'Approach':<12} {'Accuracy':<10} {'Latency (s)':<12} {'Input Tokens':<13} {'Output Tokens':<13}\")\n",
    "print(\"-\" * 70)\n",
    "for i, approach in enumerate(approaches):\n",
    "    print(f\"{approach:<12} {accuracies[i]:<10.2f} {avg_latencies[i]:<12.3f} {avg_input_tokens[i]:<13.1f} {avg_output_tokens[i]:<13.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99664374-dac0-4ca5-80b6-681167641ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "print(f\"\\nCleaning up...\")\n",
    "del model\n",
    "del tokenizer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
