{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fd7617-172f-4297-8694-381c53666049",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import gc\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from functools import partial\n",
    "from scipy.special import softmax\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import LlamaTokenizer, AutoTokenizer, AutoModelForCausalLM\n",
    "import pickle\n",
    "\n",
    "import logging\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6b1c34-ca74-4d04-b2ce-124e5312bb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CONFIG = {\n",
    "    # Dataset configuration\n",
    "    'dataset_path': 'data/SciQ/test.csv',  # Change this to your dataset path\n",
    "    'dataset_name': 'SciQ',  # For tracking purposes\n",
    "    \n",
    "    # Model configuration\n",
    "    'model_name': 'daryl149/llama-2-7b-chat-hf',\n",
    "    'torch_dtype': torch.bfloat16,\n",
    "    'device': 'cuda',\n",
    "    \n",
    "    # Evaluation configuration\n",
    "    'get_sample': False,  # Set to True to test on single sample\n",
    "    'sample_index': 0,    # Which sample to test if get_sample is True\n",
    "    'batch_size': 4,      # For STL approach\n",
    "    \n",
    "    # Prompt templates\n",
    "    'baseline_prompt': \"\"\"<s> [INST] Your task is to analyze the question and answer options A, B, C or D below.\n",
    "{query}\n",
    "[/INST] Answer:\"\"\",\n",
    "    \n",
    "    'stl_prompt': \"\"\"<s> [INST] Your task is to analyze the question and answer below. If the answer is correct, respond yes, if it is not correct respond no.\n",
    "{question} [/INST]\"\"\",\n",
    "    \n",
    "    'cot_prompt': \"\"\"<s> [INST] Your task is to analyze the question and answer options A, B, C or D below. Let's think step by step and provide your reasoning, then give your final answer in the format \"Answer: [A/B/C/D]\".\n",
    "\n",
    "{query}\n",
    "[/INST] Let me think step by step:\n",
    "\n",
    "\"\"\",\n",
    "    \n",
    "    # Generation parameters\n",
    "    'max_new_tokens_baseline': 1,\n",
    "    'max_new_tokens_stl': 1,\n",
    "    'max_new_tokens_cot': 200,\n",
    "    \n",
    "    # Output configuration\n",
    "    'save_results': True,\n",
    "    'results_dir': 'results',\n",
    "}\n",
    "\n",
    "# Create results directory\n",
    "os.makedirs(CONFIG['results_dir'], exist_ok=True)\n",
    "\n",
    "print(\"Configuration:\")\n",
    "for key, value in CONFIG.items():\n",
    "    if 'prompt' in key:\n",
    "        print(f\"  {key}: [Template defined]\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef6c8ab-b42b-492f-983b-d725e284d2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_tokenizer(model_name, torch_dtype=torch.bfloat16, device='cuda'):\n",
    "    \"\"\"Load model and tokenizer\"\"\"\n",
    "    print(f\"Loading model: {model_name}\")\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch_dtype,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Setup tokenizer for batch processing\n",
    "    tokenizer.padding_side = \"left\"\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model.config.pad_token_id = model.config.eos_token_id\n",
    "    \n",
    "    print(f\"Model loaded successfully on {device}\")\n",
    "    return model, tokenizer\n",
    "\n",
    "# Load model and tokenizer\n",
    "model, tokenizer = load_model_and_tokenizer(\n",
    "    CONFIG['model_name'], \n",
    "    CONFIG['torch_dtype'], \n",
    "    CONFIG['device']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2e8123-01c4-4882-84d9-848f55563b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_ids(tokenizer):\n",
    "    \"\"\"Get token IDs for labels and yes/no tokens\"\"\"\n",
    "    labels = [\"A\", \"B\", \"C\", \"D\"]\n",
    "    labels_token_id = tokenizer.encode(\"A B C D\")[1:]  # Remove BOS token\n",
    "    yes_token_id = tokenizer.encode(\"yes\")[1:]\n",
    "    no_token_id = tokenizer.encode(\"no\")[1:]\n",
    "    \n",
    "    print(f\"Labels token IDs: {labels_token_id}\")\n",
    "    print(f\"Yes token IDs: {yes_token_id}\")\n",
    "    print(f\"No token IDs: {no_token_id}\")\n",
    "    \n",
    "    return labels, labels_token_id, yes_token_id, no_token_id\n",
    "\n",
    "# Get token IDs\n",
    "labels, labels_token_id, yes_token_id, no_token_id = get_token_ids(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabdd542-7851-4bd8-865b-982efc6e7f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(dataset_path, get_sample=False, sample_index=0):\n",
    "    \"\"\"Load and prepare dataset\"\"\"\n",
    "    print(f\"Loading dataset from: {dataset_path}\")\n",
    "    \n",
    "    if not os.path.exists(dataset_path):\n",
    "        raise FileNotFoundError(f\"Dataset not found at {dataset_path}\")\n",
    "    \n",
    "    df = pd.read_csv(dataset_path)\n",
    "    \n",
    "    # Handle different dataset formats\n",
    "    if 'id' in df.columns:\n",
    "        df = df.drop(\"id\", axis=1)\n",
    "    \n",
    "    df.fillna(' ', inplace=True)\n",
    "    df = df.astype(str)\n",
    "    \n",
    "    if get_sample:\n",
    "        df = df.iloc[[sample_index]]\n",
    "        print(f\"Using sample at index {sample_index}\")\n",
    "    \n",
    "    # Create instruction column for baseline and CoT approaches\n",
    "    df['instruction'] = 'Question: ' + df['question'] + '\\n\\nA. ' + df['A'] + '\\n\\nB. ' + df['B'] + '\\n\\nC. ' + df['C'] + ' \\n\\nD. ' + df['D']\n",
    "    \n",
    "    print(f\"Dataset loaded: {len(df)} samples\")\n",
    "    print(\"\\nSample question:\")\n",
    "    print(f\"Question: {df['question'].iloc[0]}\")\n",
    "    print(f\"A. {df['A'].iloc[0]}\")\n",
    "    print(f\"B. {df['B'].iloc[0]}\")\n",
    "    print(f\"C. {df['C'].iloc[0]}\")\n",
    "    print(f\"D. {df['D'].iloc[0]}\")\n",
    "    print(f\"Answer: {df['answer'].iloc[0]}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load dataset\n",
    "df = load_dataset(CONFIG['dataset_path'], CONFIG['get_sample'], CONFIG['sample_index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107f985d-500a-4c7e-8ae8-bbba968ddaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens(tokenizer, text):\n",
    "    \"\"\"Count tokens in text\"\"\"\n",
    "    return len(tokenizer.encode(text))\n",
    "\n",
    "def baseline_approach(model, tokenizer, df, labels_token_id, config):\n",
    "    \"\"\"Baseline approach: Direct answer prediction\"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"BASELINE APPROACH\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    results = []\n",
    "    preds = []\n",
    "    logits_list = []\n",
    "    \n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing baseline\"):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Prepare input using configurable prompt\n",
    "        full_prompt = config['baseline_prompt'].format(query=row['instruction'])\n",
    "        input_tokens = count_tokens(tokenizer, full_prompt)\n",
    "        \n",
    "        inputs = tokenizer(full_prompt, return_tensors=\"pt\").to(f\"cuda:{model.device.index}\")\n",
    "        \n",
    "        # Generate prediction\n",
    "        with torch.no_grad():\n",
    "            output = model.generate(\n",
    "                input_ids=inputs[\"input_ids\"], \n",
    "                attention_mask=inputs[\"attention_mask\"],\n",
    "                max_new_tokens=config['max_new_tokens_baseline'],\n",
    "                return_dict_in_generate=True, \n",
    "                output_scores=True\n",
    "            )\n",
    "        \n",
    "        # Process logits\n",
    "        first_token_logits = output.scores[0][0]\n",
    "        option_logits = first_token_logits[labels_token_id].float().cpu().numpy()\n",
    "        logits_list.append(option_logits)\n",
    "        \n",
    "        # Get prediction\n",
    "        pred = np.array([\"A\", \"B\", \"C\", \"D\"])[np.argsort(option_logits)[::-1][:4]]\n",
    "        pred = ' '.join(pred)\n",
    "        preds.append(pred)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        end_time = time.time()\n",
    "        latency = end_time - start_time\n",
    "        output_tokens = config['max_new_tokens_baseline']\n",
    "        \n",
    "        # Record results\n",
    "        results.append({\n",
    "            'question_id': idx,\n",
    "            'approach': 'baseline',\n",
    "            'question': row['question'][:100] + '...' if len(row['question']) > 100 else row['question'],\n",
    "            'true_answer': row['answer'],\n",
    "            'predicted_answer': pred.split()[0],\n",
    "            'full_prediction': pred,\n",
    "            'input_tokens': input_tokens,\n",
    "            'output_tokens': output_tokens,\n",
    "            'latency_seconds': latency,\n",
    "            'logits': option_logits.tolist(),\n",
    "            'probabilities': softmax(option_logits).tolist()\n",
    "        })\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    first_preds = [pred.split()[0] for pred in preds]\n",
    "    accuracy = sum(1 for i, pred in enumerate(first_preds) if pred == df.iloc[i]['answer']) / len(df)\n",
    "    \n",
    "    print(f\"\\nBaseline Results:\")\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"Average latency: {np.mean([r['latency_seconds'] for r in results]):.3f}s\")\n",
    "    print(f\"Average input tokens: {np.mean([r['input_tokens'] for r in results]):.1f}\")\n",
    "    \n",
    "    return results, accuracy\n",
    "\n",
    "# Run baseline approach\n",
    "baseline_results, baseline_accuracy = baseline_approach(model, tokenizer, df, labels_token_id, CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860baf4f-57b0-49ba-89dc-0ca3dca04603",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stl_prompts(row, config):\n",
    "    \"\"\"Generate prompts for Single Token Logits approach\"\"\"\n",
    "    base_prompt = config['stl_prompt']\n",
    "    question = f\"\\nQuestion: {row['question']}\\nProposed answer: \"\n",
    "    \n",
    "    prompts = []\n",
    "    for letter in \"ABCD\":\n",
    "        prompt_suffix = f\"{row[letter]}\\n\\n### Response:\\n\"\n",
    "        full_prompt = base_prompt.format(question=question) + prompt_suffix\n",
    "        prompts.append(full_prompt)\n",
    "    \n",
    "    return prompts\n",
    "\n",
    "def single_token_logits_approach(model, tokenizer, df, yes_token_id, no_token_id, config):\n",
    "    \"\"\"Single Token Logits approach\"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"SINGLE TOKEN LOGITS APPROACH\") \n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Prepare all prompts\n",
    "    f = partial(get_stl_prompts, config=config)\n",
    "    all_prompts = df.apply(f, axis=1).values\n",
    "    all_prompts = [item for sublist in all_prompts for item in sublist]\n",
    "    \n",
    "    results = []\n",
    "    yes_logits_all = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(all_prompts), config['batch_size']), desc=\"Processing STL batches\"):\n",
    "            batch_start_time = time.time()\n",
    "            \n",
    "            batch = all_prompts[i:i+config['batch_size']]\n",
    "            \n",
    "            # Count input tokens for each prompt in batch\n",
    "            input_tokens_batch = [count_tokens(tokenizer, prompt) for prompt in batch]\n",
    "            \n",
    "            batch_tokens = tokenizer(batch, return_tensors=\"pt\", return_attention_mask=True, padding=True).to(f\"cuda:{model.device.index}\")\n",
    "            \n",
    "            batch_outputs = model.generate(\n",
    "                **batch_tokens,\n",
    "                max_new_tokens=config['max_new_tokens_stl'],\n",
    "                return_dict_in_generate=True,\n",
    "                output_scores=True,\n",
    "            )\n",
    "            \n",
    "            first_token_logits = batch_outputs.scores[0]\n",
    "            batch_end_time = time.time()\n",
    "            batch_latency = batch_end_time - batch_start_time\n",
    "            \n",
    "            # Process each item in batch\n",
    "            for j, scores in enumerate(first_token_logits):\n",
    "                yes_logit = float(scores[yes_token_id].float().cpu().numpy())\n",
    "                no_logit = float(scores[no_token_id].float().cpu().numpy())\n",
    "                yes_logits_all.append(yes_logit)\n",
    "                \n",
    "                # Calculate which question this belongs to\n",
    "                question_idx = (i + j) // 4\n",
    "                option_idx = (i + j) % 4\n",
    "                \n",
    "                if question_idx < len(df):  # Safety check\n",
    "                    results.append({\n",
    "                        'question_id': question_idx,\n",
    "                        'option': ['A', 'B', 'C', 'D'][option_idx],\n",
    "                        'yes_logit': yes_logit,\n",
    "                        'no_logit': no_logit,\n",
    "                        'input_tokens': input_tokens_batch[j],\n",
    "                        'output_tokens': config['max_new_tokens_stl'],\n",
    "                        'latency_seconds': batch_latency / len(batch)  # Distribute batch latency\n",
    "                    })\n",
    "            \n",
    "            # Cleanup\n",
    "            del batch_tokens\n",
    "            del batch_outputs\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    # Reshape results by question\n",
    "    yes_logits_array = np.array(yes_logits_all)\n",
    "    reshaped_logits = np.reshape(yes_logits_array, (-1, 4))\n",
    "    probs_output = np.apply_along_axis(softmax, 1, reshaped_logits)\n",
    "    \n",
    "    # Generate predictions\n",
    "    labels = np.array([\"A\", \"B\", \"C\", \"D\"])\n",
    "    preds = []\n",
    "    question_results = []\n",
    "    \n",
    "    for q_idx, (option_logits, option_probs) in enumerate(zip(reshaped_logits, probs_output)):\n",
    "        pred = labels[np.argsort(option_logits)[::-1][:4]]\n",
    "        pred = ' '.join(pred)\n",
    "        preds.append(pred)\n",
    "        \n",
    "        # Get question-level metrics\n",
    "        question_results.append({\n",
    "            'question_id': q_idx,\n",
    "            'approach': 'stl',\n",
    "            'question': df.iloc[q_idx]['question'][:100] + '...' if len(df.iloc[q_idx]['question']) > 100 else df.iloc[q_idx]['question'],\n",
    "            'true_answer': df.iloc[q_idx]['answer'],\n",
    "            'predicted_answer': pred.split()[0],\n",
    "            'full_prediction': pred,\n",
    "            'input_tokens': np.mean([r['input_tokens'] for r in results if r['question_id'] == q_idx]),\n",
    "            'output_tokens': 4,  # 4 options, 1 token each\n",
    "            'latency_seconds': sum([r['latency_seconds'] for r in results if r['question_id'] == q_idx]),\n",
    "            'yes_logits': option_logits.tolist(),\n",
    "            'probabilities': option_probs.tolist()\n",
    "        })\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    first_preds = [pred.split()[0] for pred in preds]\n",
    "    accuracy = sum(1 for i, pred in enumerate(first_preds) if pred == df.iloc[i]['answer']) / len(df)\n",
    "    \n",
    "    print(f\"\\nSTL Results:\")\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"Average latency: {np.mean([r['latency_seconds'] for r in question_results]):.3f}s\")\n",
    "    print(f\"Average input tokens: {np.mean([r['input_tokens'] for r in question_results]):.1f}\")\n",
    "    \n",
    "    return question_results, accuracy\n",
    "\n",
    "# Run STL approach\n",
    "stl_results, stl_accuracy = single_token_logits_approach(model, tokenizer, df, yes_token_id, no_token_id, CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5db170-e15e-45d3-9920-7b3e6fb7491d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_answer_from_cot(text):\n",
    "    \"\"\"Extract answer from Chain-of-Thought response\"\"\"\n",
    "    # Look for patterns like \"Answer: A\", \"Answer:A\", \"answer: B\", etc.\n",
    "    patterns = [\n",
    "        r\"Answer:\\s*([ABCD])\",\n",
    "        r\"answer:\\s*([ABCD])\",\n",
    "        r\"Answer\\s*:\\s*([ABCD])\",\n",
    "        r\"answer\\s*:\\s*([ABCD])\",\n",
    "        r\"The answer is\\s*([ABCD])\",\n",
    "        r\"the answer is\\s*([ABCD])\",\n",
    "        r\"Therefore,?\\s*([ABCD])\",\n",
    "        r\"therefore,?\\s*([ABCD])\",\n",
    "        r\"\\b([ABCD])\\s*is\\s*correct\",\n",
    "        r\"correct answer is\\s*([ABCD])\",\n",
    "        r\"([ABCD])\\s*$\"  # Single letter at end\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, text, re.IGNORECASE)\n",
    "        if match:\n",
    "            return match.group(1).upper()\n",
    "    \n",
    "    # If no pattern found, look for the most frequent A, B, C, D in the text\n",
    "    letters = re.findall(r'\\b([ABCD])\\b', text)\n",
    "    if letters:\n",
    "        from collections import Counter\n",
    "        most_common = Counter(letters).most_common(1)[0][0]\n",
    "        return most_common\n",
    "    \n",
    "    # Default fallback\n",
    "    return \"A\"\n",
    "\n",
    "def chain_of_thought_approach(model, tokenizer, df, config):\n",
    "    \"\"\"Chain-of-Thought approach: Generate reasoning then answer\"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"CHAIN-OF-THOUGHT APPROACH\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    results = []\n",
    "    preds = []\n",
    "    \n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing CoT\"):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Prepare input using configurable prompt\n",
    "        full_prompt = config['cot_prompt'].format(query=row['instruction'])\n",
    "        input_tokens = count_tokens(tokenizer, full_prompt)\n",
    "        \n",
    "        inputs = tokenizer(full_prompt, return_tensors=\"pt\").to(f\"cuda:{model.device.index}\")\n",
    "        \n",
    "        # Generate prediction with reasoning\n",
    "        with torch.no_grad():\n",
    "            output = model.generate(\n",
    "                input_ids=inputs[\"input_ids\"], \n",
    "                attention_mask=inputs[\"attention_mask\"],\n",
    "                max_new_tokens=config['max_new_tokens_cot'],\n",
    "                do_sample=True,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        # Decode the generated text\n",
    "        generated_ids = output[0][len(inputs[\"input_ids\"][0]):]\n",
    "        generated_text = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "        \n",
    "        # Extract answer from the generated text\n",
    "        predicted_answer = extract_answer_from_cot(generated_text)\n",
    "        preds.append(predicted_answer)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        end_time = time.time()\n",
    "        latency = end_time - start_time\n",
    "        output_tokens = len(generated_ids)\n",
    "        \n",
    "        # Record results\n",
    "        results.append({\n",
    "            'question_id': idx,\n",
    "            'approach': 'cot',\n",
    "            'question': row['question'][:100] + '...' if len(row['question']) > 100 else row['question'],\n",
    "            'true_answer': row['answer'],\n",
    "            'predicted_answer': predicted_answer,\n",
    "            'full_prediction': generated_text[:500] + '...' if len(generated_text) > 500 else generated_text,\n",
    "            'reasoning': generated_text,\n",
    "            'input_tokens': input_tokens,\n",
    "            'output_tokens': output_tokens,\n",
    "            'latency_seconds': latency,\n",
    "            'logits': [],  # Not applicable for CoT\n",
    "            'probabilities': []  # Not applicable for CoT\n",
    "        })\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = sum(1 for i, pred in enumerate(preds) if pred == df.iloc[i]['answer']) / len(df)\n",
    "    \n",
    "    print(f\"\\nCoT Results:\")\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"Average latency: {np.mean([r['latency_seconds'] for r in results]):.3f}s\")\n",
    "    print(f\"Average input tokens: {np.mean([r['input_tokens'] for r in results]):.1f}\")\n",
    "    print(f\"Average output tokens: {np.mean([r['output_tokens'] for r in results]):.1f}\")\n",
    "    \n",
    "    return results, accuracy\n",
    "\n",
    "# Run CoT approach\n",
    "cot_results, cot_accuracy = chain_of_thought_approach(model, tokenizer, df, CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bea85cc-e26b-4b3b-95ac-c0c7100b8d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(baseline_results, stl_results, cot_results, config):\n",
    "    \"\"\"Save results to files\"\"\"\n",
    "    if not config['save_results']:\n",
    "        return\n",
    "    \n",
    "    # Combine all results\n",
    "    all_results = baseline_results + stl_results + cot_results\n",
    "    results_df = pd.DataFrame(all_results)\n",
    "    \n",
    "    # Create filename with model and dataset info\n",
    "    model_name = config['model_name'].replace('/', '_')\n",
    "    dataset_name = config['dataset_name']\n",
    "    timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    filename = f\"{config['results_dir']}/results_{model_name}_{dataset_name}_{timestamp}.csv\"\n",
    "    results_df.to_csv(filename, index=False)\n",
    "    \n",
    "    print(f\"\\nResults saved to: {filename}\")\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\nSummary:\")\n",
    "    print(f\"Dataset: {dataset_name}\")\n",
    "    print(f\"Model: {config['model_name']}\")\n",
    "    print(f\"Total questions: {len(baseline_results)}\")\n",
    "    print(f\"Baseline accuracy: {baseline_accuracy * 100:.2f}%\")\n",
    "    print(f\"STL accuracy: {stl_accuracy * 100:.2f}%\")\n",
    "    print(f\"CoT accuracy: {cot_accuracy * 100:.2f}%\")\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "# Save results\n",
    "results_df = save_results(baseline_results, stl_results, cot_results, CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eca860a-1639-4a35-a8da-5ea561c640de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample results\n",
    "if len(baseline_results) > 0:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"SAMPLE RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    sample_idx = 0\n",
    "    baseline_sample = baseline_results[sample_idx]\n",
    "    stl_sample = stl_results[sample_idx]\n",
    "    cot_sample = cot_results[sample_idx]\n",
    "    \n",
    "    print(f\"\\nQuestion: {baseline_sample['question']}\")\n",
    "    print(f\"True Answer: {baseline_sample['true_answer']}\")\n",
    "    \n",
    "    print(f\"\\nBaseline:\")\n",
    "    print(f\"  Prediction: {baseline_sample['predicted_answer']}\")\n",
    "    print(f\"  Latency: {baseline_sample['latency_seconds']:.3f}s\")\n",
    "    print(f\"  Input tokens: {baseline_sample['input_tokens']}\")\n",
    "    print(f\"  Probabilities: {[f'{p:.3f}' for p in baseline_sample['probabilities']]}\")\n",
    "    \n",
    "    print(f\"\\nSTL:\")\n",
    "    print(f\"  Prediction: {stl_sample['predicted_answer']}\")\n",
    "    print(f\"  Latency: {stl_sample['latency_seconds']:.3f}s\")\n",
    "    print(f\"  Input tokens: {stl_sample['input_tokens']:.1f}\")\n",
    "    print(f\"  Probabilities: {[f'{p:.3f}' for p in stl_sample['probabilities']]}\")\n",
    "    \n",
    "    print(f\"\\nCoT:\")\n",
    "    print(f\"  Prediction: {cot_sample['predicted_answer']}\")\n",
    "    print(f\"  Latency: {cot_sample['latency_seconds']:.3f}s\")\n",
    "    print(f\"  Input/Output tokens: {cot_sample['input_tokens']}/{cot_sample['output_tokens']}\")\n",
    "    print(f\"  Reasoning: {cot_sample['reasoning'][:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c58478-491b-42f2-8e28-8d1b04dbee15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance comparison\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PERFORMANCE COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "approaches = ['Baseline', 'STL', 'CoT']\n",
    "accuracies = [baseline_accuracy * 100, stl_accuracy * 100, cot_accuracy * 100]\n",
    "avg_latencies = [\n",
    "    np.mean([r['latency_seconds'] for r in baseline_results]),\n",
    "    np.mean([r['latency_seconds'] for r in stl_results]),\n",
    "    np.mean([r['latency_seconds'] for r in cot_results])\n",
    "]\n",
    "avg_input_tokens = [\n",
    "    np.mean([r['input_tokens'] for r in baseline_results]),\n",
    "    np.mean([r['input_tokens'] for r in stl_results]),\n",
    "    np.mean([r['input_tokens'] for r in cot_results])\n",
    "]\n",
    "avg_output_tokens = [\n",
    "    np.mean([r['output_tokens'] for r in baseline_results]),\n",
    "    np.mean([r['output_tokens'] for r in stl_results]),\n",
    "    np.mean([r['output_tokens'] for r in cot_results])\n",
    "]\n",
    "\n",
    "print(f\"{'Approach':<12} {'Accuracy':<10} {'Latency (s)':<12} {'Input Tokens':<13} {'Output Tokens':<13}\")\n",
    "print(\"-\" * 70)\n",
    "for i, approach in enumerate(approaches):\n",
    "    print(f\"{approach:<12} {accuracies[i]:<10.2f} {avg_latencies[i]:<12.3f} {avg_input_tokens[i]:<13.1f} {avg_output_tokens[i]:<13.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99664374-dac0-4ca5-80b6-681167641ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "print(f\"\\nCleaning up...\")\n",
    "del model\n",
    "del tokenizer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
