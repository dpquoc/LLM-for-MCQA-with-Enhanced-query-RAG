{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kItdQWB7QEMU"
      },
      "source": [
        "# Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "execution": {
          "iopub.execute_input": "2024-02-02T11:04:16.873982Z",
          "iopub.status.busy": "2024-02-02T11:04:16.873431Z",
          "iopub.status.idle": "2024-02-02T11:05:04.273466Z",
          "shell.execute_reply": "2024-02-02T11:05:04.271870Z",
          "shell.execute_reply.started": "2024-02-02T11:04:16.873933Z"
        },
        "id": "5uThuhhJQDll",
        "outputId": "2343dbbf-2717-4ba4-de21-acab382e08e6",
        "scrolled": true,
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: faiss-gpu in /opt/conda/lib/python3.10/site-packages (1.7.2)\n",
            "Requirement already satisfied: sentence_transformers in /opt/conda/lib/python3.10/site-packages (2.3.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.32.0 in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (4.35.0)\n",
            "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (4.66.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (2.0.0+cpu)\n",
            "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (1.24.3)\n",
            "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (1.11.3)\n",
            "Requirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (3.2.4)\n",
            "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (0.1.99)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (0.17.3)\n",
            "Requirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (10.1.0)\n",
            "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (3.12.2)\n",
            "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2023.10.0)\n",
            "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (21.3)\n",
            "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (3.1)\n",
            "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (3.1.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.32.0->sentence_transformers) (2023.8.8)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.32.0->sentence_transformers) (0.14.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.32.0->sentence_transformers) (0.4.0)\n",
            "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from nltk->sentence_transformers) (1.16.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence_transformers) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence_transformers) (3.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.15.1->sentence_transformers) (3.0.9)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install faiss-gpu\n",
        "!pip install sentence_transformers\n",
        "import faiss\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import os\n",
        "import gc\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import subprocess\n",
        "\n",
        "from IPython.display import FileLink, display"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ij7MBIWJQKnr"
      },
      "source": [
        "# Download Resources"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "na46mTzsQMkT"
      },
      "outputs": [],
      "source": [
        "os.makedirs('dpq-wiki-parsed', exist_ok=True)\n",
        "os.chdir('dpq-wiki-parsed')\n",
        "\n",
        "file_names = [\n",
        "    'a.parquet',\n",
        "    'b.parquet',\n",
        "    'c.parquet',\n",
        "    'chunk_index.parquet',\n",
        "    'd.parquet',\n",
        "    'e.parquet',\n",
        "    'f.parquet',\n",
        "    'g.parquet',\n",
        "    'h.parquet',\n",
        "    'i.parquet',\n",
        "    'j.parquet',\n",
        "    'k.parquet',\n",
        "    'l.parquet',\n",
        "    'm.parquet',\n",
        "    'n.parquet',\n",
        "    'number.parquet',\n",
        "    'o.parquet',\n",
        "    'p.parquet',\n",
        "    'q.parquet',\n",
        "    'r.parquet',\n",
        "    's.parquet',\n",
        "    't.parquet',\n",
        "    'u.parquet',\n",
        "    'v.parquet',\n",
        "    'w.parquet',\n",
        "    'wiki_index.parquet',\n",
        "    'x.parquet',\n",
        "    'y.parquet',\n",
        "    'z.parquet'\n",
        "]\n",
        "repo_id = \"dpquoc/wiki-parsed\"\n",
        "\n",
        "# Create a list to store subprocess.Popen objects\n",
        "processes = []\n",
        "\n",
        "for file_name in file_names:\n",
        "    link = f'https://huggingface.co/datasets/{repo_id}/resolve/main/{file_name}'\n",
        "    command = [\"wget\", link]\n",
        "\n",
        "    # Redirect output to /dev/null (Linux) or NUL (Windows)\n",
        "    processes.append(subprocess.Popen(command, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL))\n",
        "\n",
        "# Wait for all processes to complete\n",
        "for process in processes:\n",
        "    process.wait()\n",
        "\n",
        "print(\"All downloads initiated.\")\n",
        "os.chdir('..')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9h-V38pjQOby"
      },
      "outputs": [],
      "source": [
        "os.makedirs('bge-small-en', exist_ok=True)\n",
        "os.chdir('bge-small-en')\n",
        "\n",
        "file_names = [\n",
        "    'config.json',\n",
        "    'config_sentence_transformers.json',\n",
        "    'modules.json',\n",
        "    'pytorch_model.bin',\n",
        "    'sentence_bert_config.json',\n",
        "    'special_tokens_map.json',\n",
        "    'tokenizer.json',\n",
        "    'tokenizer_config.json',\n",
        "    'vocab.txt'\n",
        "]\n",
        "\n",
        "repo_id = \"BAAI/bge-small-en-v1.5\"\n",
        "\n",
        "# Create a list to store subprocess.Popen objects\n",
        "processes = []\n",
        "\n",
        "for file_name in file_names:\n",
        "    link = f'https://huggingface.co/{repo_id}/resolve/main/{file_name}'\n",
        "    command = [\"wget\", link]\n",
        "\n",
        "    # Redirect output to /dev/null (Linux) or NUL (Windows)\n",
        "    processes.append(subprocess.Popen(command, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL))\n",
        "\n",
        "# Wait for all processes to complete\n",
        "for process in processes:\n",
        "    process.wait()\n",
        "\n",
        "print(\"All downloads initiated.\")\n",
        "\n",
        "folder_name = '1_Pooling'\n",
        "folder_path = os.path.join(os.getcwd(), folder_name)\n",
        "os.makedirs(folder_path, exist_ok=True)\n",
        "os.chdir(folder_path)\n",
        "!wget https://huggingface.co/BAAI/bge-small-en-v1.5/resolve/main/1_Pooling/config.json\n",
        "os.chdir('..')\n",
        "os.chdir('..')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dnbq5IWPQP8L"
      },
      "source": [
        "# Create Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qGnHIp62QDln",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "from huggingface_hub import HfApi\n",
        "login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qkMJazMvQDln",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "base_path = '/content/dpq-wiki-parsed/'\n",
        "\n",
        "file_names = [\n",
        "    'a.parquet',\n",
        "    'b.parquet',\n",
        "    'c.parquet',\n",
        "    'd.parquet',\n",
        "    'e.parquet',\n",
        "    'f.parquet',\n",
        "    'g.parquet',\n",
        "    'h.parquet',\n",
        "    'i.parquet',\n",
        "    'j.parquet',\n",
        "    'k.parquet',\n",
        "    'l.parquet',\n",
        "    'm.parquet',\n",
        "    'n.parquet',\n",
        "    'number.parquet',\n",
        "    'o.parquet',\n",
        "    'p.parquet',\n",
        "    'q.parquet',\n",
        "    'r.parquet',\n",
        "    's.parquet',\n",
        "    't.parquet',\n",
        "    'u.parquet',\n",
        "    'v.parquet',\n",
        "    'w.parquet',\n",
        "    'x.parquet',\n",
        "    'y.parquet',\n",
        "    'z.parquet'\n",
        "]\n",
        "\n",
        "model_embedding = SentenceTransformer('/content/bge-small-en', device=\"cuda:0\")\n",
        "faiss_index_path = \"/content/wikipedia_embeddings.index\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3GdVWi11QDln",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def chunk_split(text, chunk_size=100, stride=90):\n",
        "    # Split text into words\n",
        "    words = text.split()\n",
        "    chunks = []\n",
        "    for i in range(0, len(words), stride):\n",
        "        if i + chunk_size <= len(words):\n",
        "            # Full chunk\n",
        "            chunks.append(' '.join(words[i:i + chunk_size]))\n",
        "        else:\n",
        "            # Last chunk, which might be smaller\n",
        "            chunks.append(' '.join(words[i:len(words)]))\n",
        "    return chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wdt3_4V4QDlo",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# GET ALL TEXT CHUNKS\n",
        "all_chunks = []\n",
        "\n",
        "for file_name in file_names:\n",
        "    # Construct the full path\n",
        "    full_path = os.path.join(base_path, file_name)\n",
        "\n",
        "    # Read the file into a DataFrame\n",
        "    df = pd.read_parquet(full_path)\n",
        "    print(f\"Processing file_name: {file_name} ......\")\n",
        "\n",
        "    df['chunks'] = df.text.apply(chunk_split)\n",
        "    chunks = [chunk for sublist in df['chunks'].tolist() for chunk in sublist]\n",
        "    all_chunks.extend(chunks)\n",
        "    del df, chunks\n",
        "    gc.collect()\n",
        "\n",
        "print(f\"The final shape of the array 'all_chunks' is: {len(all_chunks)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mZM7vlSjQDlo",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "for file_name in file_names:\n",
        "    # Construct the full path\n",
        "    full_path = os.path.join(base_path, file_name)\n",
        "    real_name, _ = os.path.splitext(file_name)\n",
        "\n",
        "\n",
        "    # Read the file into a DataFrame\n",
        "    df = pd.read_parquet(full_path)\n",
        "    print(f\"Processing file_name: {file_name} ......\")\n",
        "\n",
        "    df['chunks'] = df.text.apply(chunk_split)\n",
        "    chunks = [chunk for sublist in df['chunks'].tolist() for chunk in sublist]\n",
        "    del df\n",
        "\n",
        "    embeddings = model_embedding.encode(chunks,\n",
        "                                        batch_size=64,\n",
        "                                        convert_to_tensor=False,\n",
        "                                        convert_to_numpy=True,\n",
        "                                        normalize_embeddings=True)\n",
        "\n",
        "    del chunks # free some memory\n",
        "\n",
        "    file_name_hug = real_name + \".npy\"\n",
        "    np.save(file_name_hug, embeddings)\n",
        "    print(file_name_hug, embeddings.shape)\n",
        "\n",
        "    del embeddings\n",
        "\n",
        "    api = HfApi()\n",
        "\n",
        "    api.upload_file(\n",
        "        path_or_fileobj=\"/content/\" + file_name_hug,\n",
        "        path_in_repo=file_name,\n",
        "        repo_id=\"dpquoc/np-chunks\",\n",
        "        repo_type=\"dataset\",\n",
        "    )\n",
        "    os.remove(\"/content/\" + file_name_hug)\n",
        "    gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L53EO3wrQDlo",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "document_embeddings.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M6fr0JGpQDlp"
      },
      "outputs": [],
      "source": [
        "document_embeddings = np.array(document_embeddings)\n",
        "index = faiss.IndexFlatL2(document_embeddings.shape[1])\n",
        "index.add(document_embeddings)\n",
        "faiss.write_index(index, faiss_index_path)\n",
        "print(f\"Faiss Index Successfully Saved to '{faiss_index_path}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "idwBSCXfQDlp",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "document_embeddings = []\n",
        "document_embeddings.extend(embeddings)\n",
        "\n",
        "document_embeddings = np.array(document_embeddings)\n",
        "index = faiss.IndexFlatL2(document_embeddings.shape[1])\n",
        "index.add(document_embeddings)\n",
        "faiss.write_index(index, faiss_index_path)\n",
        "print(f\"Faiss Index Successfully Saved to '{faiss_index_path}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4g2Iyh41QDlp",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SUNy2Rn3QDlp"
      },
      "outputs": [],
      "source": [
        "document_embeddings = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uca3GIG_QDlq",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "arrays = []\n",
        "\n",
        "for file_name in file_names:\n",
        "    array = np.load(file_name)\n",
        "    arrays.append(array)\n",
        "\n",
        "all_data = np.concatenate(arrays)\n",
        "del arrays , array\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UbE7OMTSQDlq",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Assuming all_data is your final numpy array\n",
        "# Sample 20% of the data for training\n",
        "n_train = int(0.2 * all_data.shape[0])\n",
        "train_data = np.random.permutation(all_data)[:n_train]\n",
        "\n",
        "# Dimension of the vectors\n",
        "d = 384\n",
        "print(d)\n",
        "\n",
        "# Number of centroids\n",
        "k = 256\n",
        "\n",
        "# Create the quantizer\n",
        "quantizer = faiss.IndexFlatL2(d)\n",
        "\n",
        "# Create the index\n",
        "index = faiss.IndexIVFPQ(quantizer, d, k, 96, 8)  # 96 bytes -> 12 * 8 bits\n",
        "\n",
        "# Train the index\n",
        "index.train(train_data)\n",
        "\n",
        "# Add all the vectors to the index\n",
        "index.add(all_data)\n",
        "faiss.write_index(index, 'chunk_index.faiss')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8uCIJn_IQDlq",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "faiss.write_index(index, 'quantized_index.faiss')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FRt-cHe_QDlq",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "n = index.ntotal\n",
        "\n",
        "print(f\"The index contains {n} vectors.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEZoggyzQDlq"
      },
      "source": [
        "# FINAL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-02T11:05:04.280140Z",
          "iopub.status.busy": "2024-02-02T11:05:04.279456Z",
          "iopub.status.idle": "2024-02-02T11:05:04.297671Z",
          "shell.execute_reply": "2024-02-02T11:05:04.296045Z",
          "shell.execute_reply.started": "2024-02-02T11:05:04.280102Z"
        },
        "id": "5TmLHcxoQDls",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import pandas as pd\n",
        "import os\n",
        "import gc\n",
        "import numpy as np\n",
        "import faiss\n",
        "from huggingface_hub import login, HfApi\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-02T11:05:04.300435Z",
          "iopub.status.busy": "2024-02-02T11:05:04.299836Z",
          "iopub.status.idle": "2024-02-02T11:05:04.529593Z",
          "shell.execute_reply": "2024-02-02T11:05:04.527952Z",
          "shell.execute_reply.started": "2024-02-02T11:05:04.300381Z"
        },
        "id": "bohmGTPCQDls",
        "outputId": "87071f13-bb30-493e-c07c-037ca0cfc233",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token is valid (permission: write).\n",
            "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
            "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
            "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
            "\n",
            "git config --global credential.helper store\n",
            "\n",
            "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
            "Token has not been saved to git credential helper.\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "login()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-02T09:55:57.434037Z",
          "iopub.status.busy": "2024-02-02T09:55:57.433302Z",
          "iopub.status.idle": "2024-02-02T10:10:02.930286Z",
          "shell.execute_reply": "2024-02-02T10:10:02.925038Z",
          "shell.execute_reply.started": "2024-02-02T09:55:57.433996Z"
        },
        "id": "IW30Yk7KQDls",
        "outputId": "13ab6a6d-ae9b-4b8f-f28f-fab54e1d30de",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All downloads initiated.\n"
          ]
        }
      ],
      "source": [
        "os.chdir(\"..\")\n",
        "os.mkdir(\"tmp\")\n",
        "os.chdir(\"tmp\")\n",
        "\n",
        "file_names = [\n",
        "    \"a.npy\",\n",
        "    \"b.parquet\",\n",
        "    \"c.parquet\",\n",
        "    \"d_e_f.npy\",\n",
        "    \"g.parquet\",\n",
        "    \"h.parquet\",\n",
        "    \"i.parquet\",\n",
        "    \"j.parquet\",\n",
        "    \"k.parquet\",\n",
        "    \"l.parquet\",\n",
        "    \"m.parquet\",\n",
        "    \"n.parquet\",\n",
        "    \"number.parquet\",\n",
        "    \"o.parquet\",\n",
        "    \"p.parquet\",\n",
        "    \"q.parquet\",\n",
        "    \"r.parquet\",\n",
        "    \"s.parquet\",\n",
        "    \"t.parquet\",\n",
        "    \"u.parquet\",\n",
        "    \"v.parquet\",\n",
        "    \"w.parquet\",\n",
        "    \"x.parquet\",\n",
        "    \"y.parquet\",\n",
        "    \"z.parquet\"\n",
        "]\n",
        "repo_id = \"dpquoc/np-chunks\"\n",
        "\n",
        "# Create a list to store subprocess.Popen objects\n",
        "processes = []\n",
        "\n",
        "for file_name in file_names:\n",
        "    link = f'https://huggingface.co/datasets/{repo_id}/resolve/main/{file_name}'\n",
        "    command = [\"wget\", link]\n",
        "\n",
        "    # Redirect output to /dev/null (Linux) or NUL (Windows)\n",
        "    processes.append(subprocess.Popen(command, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL))\n",
        "\n",
        "# Wait for all processes to complete\n",
        "for process in processes:\n",
        "    process.wait()\n",
        "\n",
        "print(\"All downloads initiated.\")\n",
        "\n",
        "os.chdir(\"..\")\n",
        "os.chdir(\"working\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-02T11:05:04.533526Z",
          "iopub.status.busy": "2024-02-02T11:05:04.532915Z",
          "iopub.status.idle": "2024-02-02T11:05:04.542682Z",
          "shell.execute_reply": "2024-02-02T11:05:04.541118Z",
          "shell.execute_reply.started": "2024-02-02T11:05:04.533472Z"
        },
        "id": "UkMjbWacQDlt",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "file_names = [\n",
        "    \"a.npy\",\n",
        "    \"b.parquet\",\n",
        "    \"c.parquet\",\n",
        "    \"d_e_f.npy\",\n",
        "    \"g.parquet\",\n",
        "    \"h.parquet\",\n",
        "    \"i.parquet\",\n",
        "    \"j.parquet\",\n",
        "    \"k.parquet\",\n",
        "    \"l.parquet\",\n",
        "    \"m.parquet\",\n",
        "    \"n.parquet\",\n",
        "    \"number.parquet\",\n",
        "    \"o.parquet\",\n",
        "    \"p.parquet\",\n",
        "    \"q.parquet\",\n",
        "    \"r.parquet\",\n",
        "    \"s.parquet\",\n",
        "    \"t.parquet\",\n",
        "    \"u.parquet\",\n",
        "    \"v.parquet\",\n",
        "    \"w.parquet\",\n",
        "    \"x.parquet\",\n",
        "    \"y.parquet\",\n",
        "    \"z.parquet\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-02T10:22:50.804368Z",
          "iopub.status.busy": "2024-02-02T10:22:50.803706Z",
          "iopub.status.idle": "2024-02-02T10:34:49.018357Z",
          "shell.execute_reply": "2024-02-02T10:34:49.016734Z",
          "shell.execute_reply.started": "2024-02-02T10:22:50.804317Z"
        },
        "id": "VrKXE4LuQDlt",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "arrays = np.array([])\n",
        "\n",
        "for file_name in file_names:\n",
        "    # Load the array from the file\n",
        "    array = np.load(\"/content/tmp/\" + file_name)\n",
        "\n",
        "    # Get 20% of the elements randomly\n",
        "    sample_size = int(0.1 * array.shape[0])\n",
        "    sample = np.random.permutation(array)[:sample_size]\n",
        "    del array\n",
        "\n",
        "    # Add the sample to the numpy array\n",
        "    if arrays.size == 0:\n",
        "        arrays = sample\n",
        "    else:\n",
        "        arrays = np.vstack((arrays, sample))\n",
        "    del sample\n",
        "    gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-02T10:39:11.644331Z",
          "iopub.status.busy": "2024-02-02T10:39:11.641371Z",
          "iopub.status.idle": "2024-02-02T10:39:19.487468Z",
          "shell.execute_reply": "2024-02-02T10:39:19.486072Z",
          "shell.execute_reply.started": "2024-02-02T10:39:11.644192Z"
        },
        "id": "HVqtYl1BQDlt",
        "outputId": "90e65b38-2f94-4201-8443-97280c44aded",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "574"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "arrays = arrays.astype('float32').reshape(-1, d)\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-02T10:39:27.450313Z",
          "iopub.status.busy": "2024-02-02T10:39:27.449797Z",
          "iopub.status.idle": "2024-02-02T10:41:05.861975Z",
          "shell.execute_reply": "2024-02-02T10:41:05.860317Z",
          "shell.execute_reply.started": "2024-02-02T10:39:27.450274Z"
        },
        "id": "Ly1_LGcGQDlt",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Dimension of the vectors\n",
        "d = 384\n",
        "\n",
        "# Number of centroids\n",
        "k = 256\n",
        "\n",
        "# Create the quantizer\n",
        "quantizer = faiss.IndexFlatL2(d)\n",
        "\n",
        "# Create the index\n",
        "index = faiss.IndexIVFPQ(quantizer, d, k, 96, 8)  # 96 bytes -> 12 * 8 bits\n",
        "\n",
        "# Train the index\n",
        "index.train(arrays)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-02T10:42:22.605935Z",
          "iopub.status.busy": "2024-02-02T10:42:22.605311Z",
          "iopub.status.idle": "2024-02-02T10:42:22.631646Z",
          "shell.execute_reply": "2024-02-02T10:42:22.630542Z",
          "shell.execute_reply.started": "2024-02-02T10:42:22.605883Z"
        },
        "id": "Az2C3r5wQDlt",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "del arrays"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-02T10:42:40.335775Z",
          "iopub.status.busy": "2024-02-02T10:42:40.335228Z",
          "iopub.status.idle": "2024-02-02T10:42:40.700291Z",
          "shell.execute_reply": "2024-02-02T10:42:40.698790Z",
          "shell.execute_reply.started": "2024-02-02T10:42:40.335708Z"
        },
        "id": "ogYGGfItQDlt",
        "outputId": "bc08c085-43d6-408e-8029-d16211c0dcfa",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-02T10:46:51.059871Z",
          "iopub.status.busy": "2024-02-02T10:46:51.059350Z",
          "iopub.status.idle": "2024-02-02T10:46:51.078893Z",
          "shell.execute_reply": "2024-02-02T10:46:51.077521Z",
          "shell.execute_reply.started": "2024-02-02T10:46:51.059831Z"
        },
        "id": "-Dj6Nl3iQDlu",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "faiss.write_index(index, 'trained.index')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-02T11:05:42.547992Z",
          "iopub.status.busy": "2024-02-02T11:05:42.547385Z",
          "iopub.status.idle": "2024-02-02T11:05:42.637156Z",
          "shell.execute_reply": "2024-02-02T11:05:42.635625Z",
          "shell.execute_reply.started": "2024-02-02T11:05:42.547945Z"
        },
        "id": "hILwEhD0QDlu",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Later, you can load the index from the file\n",
        "index = faiss.read_index('trained.index')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-02T11:08:05.555699Z",
          "iopub.status.busy": "2024-02-02T11:08:05.555200Z",
          "iopub.status.idle": "2024-02-02T11:08:05.873677Z",
          "shell.execute_reply": "2024-02-02T11:08:05.872151Z",
          "shell.execute_reply.started": "2024-02-02T11:08:05.555664Z"
        },
        "id": "_SZFhgS_QDlu",
        "outputId": "e8e7f75c-d000-4090-9aa4-a0793b42f86d",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1465"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "del array\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-02T11:08:11.750003Z",
          "iopub.status.busy": "2024-02-02T11:08:11.749502Z",
          "iopub.status.idle": "2024-02-02T12:05:02.485425Z",
          "shell.execute_reply": "2024-02-02T12:05:02.483234Z",
          "shell.execute_reply.started": "2024-02-02T11:08:11.749965Z"
        },
        "id": "GAYnPugrQDlu",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "for file_name in file_names:\n",
        "    # Load the array from the file\n",
        "    array = np.load(\"/content/tmp/\" + file_name)\n",
        "    array = array.astype('float32')\n",
        "    index.add(array)\n",
        "    del array\n",
        "    gc.collect()\n",
        "\n",
        "faiss.write_index(index, 'chunk_faiss.index')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "api = HfApi()\n",
        "\n",
        "api.upload_file(\n",
        "    path_or_fileobj= 'chunk_faiss.index',\n",
        "    path_in_repo='chunk_faiss.index',\n",
        "    repo_id=\"dpquoc/wiki-faiss-index\",\n",
        "    repo_type=\"dataset\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-02T12:08:16.207760Z",
          "iopub.status.busy": "2024-02-02T12:08:16.206530Z",
          "iopub.status.idle": "2024-02-02T12:08:16.216362Z",
          "shell.execute_reply": "2024-02-02T12:08:16.214950Z",
          "shell.execute_reply.started": "2024-02-02T12:08:16.207689Z"
        },
        "id": "1OgWz2Z1QDlv",
        "outputId": "88b170a0-bc9b-4f76-bbd7-8776fba683ca",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "37573660"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "index.ntotal"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
